{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ens3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTVVNjHb41BH"
      },
      "source": [
        "!pip install --q flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzXsPGvg5FNP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht4Piofe5K_9"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.embeddings import StackedEmbeddings, TransformerDocumentEmbeddings, DocumentPoolEmbeddings, FlairEmbeddings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIyxH5Mh5XTq"
      },
      "source": [
        "filename =r'/Dataset/GermEval21_Toxic_Train/GermEval21_Toxic_Train.csv'\n",
        "data=pd.read_csv(filename)\n",
        "# df1=data[['comment_text','Sub1_Toxic']]\n",
        "# df2=data[['comment_text','Sub2_Engaging']]\n",
        "# df3=data[['comment_text','Sub3_FactClaiming']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Fdhh9y590N"
      },
      "source": [
        "#initialize transformer embeddings -roberta\n",
        "transformer_embedding = TransformerDocumentEmbeddings('xlm-roberta-base')  #xlm-large is too big for memory\n",
        "\n",
        "#initialize transformer embeddings -German bert\n",
        "transformer_embedding_1 = TransformerDocumentEmbeddings('bert-base-german-cased')  #xlm-large is too big for memory\n",
        "\n",
        "\n",
        "#initialize flair embeddings\n",
        "flair_embedding_forward = FlairEmbeddings('de-forward');\n",
        "flair_embedding_backward = FlairEmbeddings('de-backward');\n",
        "document_embeddings = DocumentPoolEmbeddings([flair_embedding_forward, flair_embedding_backward]) ;\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKssNcET6VDd"
      },
      "source": [
        "df1['flair_embeddings']= \" \"\n",
        "#create features for tweets_training for Model 2.2 FLair embeddings\n",
        "for i in range(0,len(df1)):\n",
        "\n",
        "    # create a sentence\n",
        "    sentence = Sentence(df1['comment_text'][i])\n",
        "\n",
        "    # embed the sentence\n",
        "    document_embeddings.embed(sentence)\n",
        "\n",
        "    embedding = sentence.embedding.cpu()\n",
        "    \n",
        "    #save vector as numpy\n",
        "    embedding = embedding.detach().numpy()\n",
        "    \n",
        "    #save vector as pandas dataframe\n",
        "    embedding = pd.DataFrame(embedding)\n",
        "    \n",
        "    #make list out of sentence\n",
        "    embedding = embedding[0].tolist()\n",
        "\n",
        "    #add the embedding vector to the column of stacked embeddings\n",
        "    df1['flair_embeddings'][i] = embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpUVpy1P6ZUh"
      },
      "source": [
        "df1['roberta_embeddings']= \" \"\n",
        "for i in range(0,len(df1)):\n",
        "\n",
        "    # create a sentence\n",
        "    sentence = Sentence(df1['comment_text'][i])\n",
        "\n",
        "    # embed the sentence\n",
        "    transformer_embedding.embed(sentence)\n",
        "\n",
        "    embedding =sentence.embedding.cpu()\n",
        "\n",
        "    #save vector as numpy\n",
        "    embedding = embedding.detach().numpy()\n",
        "    \n",
        "    # #save vector as pandas dataframe\n",
        "    embedding = pd.DataFrame(embedding)\n",
        "    \n",
        "    # #make list out of sentence\n",
        "    embedding = embedding[0].tolist()\n",
        "\n",
        "    # #add the embedding vector to the column of stacked embeddings\n",
        "    df1['roberta_embeddings'][i] = embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZsnzXk-6zz5"
      },
      "source": [
        "train['gbert_embeddings'] =''\n",
        "for i in range(0,len(train)):\n",
        "\n",
        "    # create a sentence\n",
        "    sentence = Sentence(train['comment_text'].iloc[i])\n",
        "\n",
        "    # embed the sentence\n",
        "    transformer_embedding_1.embed(sentence)\n",
        "\n",
        "    embedding =sentence.embedding.cpu()\n",
        "\n",
        "    #save vector as numpy\n",
        "    embedding = embedding.detach().numpy()\n",
        "    \n",
        "    # #save vector as pandas dataframe\n",
        "    embedding = pd.DataFrame(embedding)\n",
        "    \n",
        "    # #make list out of sentence\n",
        "    embedding = embedding[0].tolist()\n",
        "\n",
        "    # #add the embedding vector to the column of stacked embeddings\n",
        "    train['gbert_embeddings'].iloc[i] = embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD7UBnTlmC4Y"
      },
      "source": [
        "model1 = svm.SVC(C=1, kernel='linear', gamma=1)\n",
        "model2 = svm.SVC(C=1, kernel='linear', gamma=1)\n",
        "model3 = svm.SVC(C=1, kernel='linear', gamma=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu7_FqBUnxe8"
      },
      "source": [
        "model1.fit(list(df1['flair_embeddings']),df1['Sub1_Toxic'])\n",
        "model2.fit(list(df1['roberta_embeddings']),df1['Sub1_Toxic'])\n",
        "model3.fit(list(df1['gbert_embeddings']),df1['Sub1_Toxic'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQaKUzJXnsXU"
      },
      "source": [
        "model1_pred = model1.predict(list(df1['flair_embeddings']))\n",
        "model2_pred = model2.predict(list(df1['roberta_embeddings']))\n",
        "model3_pred= model3.predict(list(df1['gbert_embeddings']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYHRiZAgosku"
      },
      "source": [
        "df1['flair_predictions']=model1_pred\n",
        "df1['roberta_predictions']=model2_pred\n",
        "df1['gbert_predictions']=model3_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHrOf24spSRu"
      },
      "source": [
        "print('Metrics for flair\\n')\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(df1['Sub1_Toxic'], df1['flair_predictions'])\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(df1['Sub1_Toxic'], df1['flair_predictions'] ,average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(df1['Sub1_Toxic'], df1['flair_predictions'] ,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['flair_predictions'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Y2kMBJpTaL"
      },
      "source": [
        "print('Metrics for roberta\\n')\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(df1['Sub1_Toxic'], df1['roberta_predictions'])\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(df1['Sub1_Toxic'], df1['roberta_predictions'] ,average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(df1['Sub1_Toxic'], df1['roberta_predictions'] ,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['roberta_predictions'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35pNf98DpUTY"
      },
      "source": [
        "# accuracy: (tp + tn) / (p + n)\n",
        "print('Metrics for gbert\\n')\n",
        "accuracy = accuracy_score(df1['Sub1_Toxic'], df1['gbert_predictions'])\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(df1['Sub1_Toxic'], df1['gbert_predictions'] ,average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(df1['Sub1_Toxic'], df1['gbert_predictions'] ,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['gbert_predictions'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl_sWMtvqXp_"
      },
      "source": [
        "#Metrics for Ensemble\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "print('Metrics for Ensemble\\n')\n",
        "accuracy = accuracy_score(df1['Sub1_Toxic'], df1['Ensemble'] )\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)'Sub1_Toxic'\n",
        "precision = precision_score(df1['Sub1_Toxic'], df1['Ensemble'] ,average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(df1['Sub1_Toxic'], df1['Ensemble'] ,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['Ensemble'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}