{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ens1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kfZx7FA6rk2"
      },
      "source": [
        "#  you need to install tensorflow-text,tf-models-official\n",
        "!pip install -q tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAWm1gyl6taN"
      },
      "source": [
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ3QFSzdKbbh"
      },
      "source": [
        "!pip install --q flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg8OoX7Y6-Dv"
      },
      "source": [
        "# if running from colabs, mount drive and upload the files in the google drive otherwise comment out\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOluyaNi6F69"
      },
      "source": [
        "#import packages\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow import feature_column\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization \n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from matplotlib import pyplot\n",
        "from flair.data import Sentence\n",
        "from flair.embeddings import StackedEmbeddings, TransformerDocumentEmbeddings, DocumentPoolEmbeddings, FlairEmbeddings\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFT6JIXaLBad"
      },
      "source": [
        "#upload dataset\n",
        "filename =r'/Dataset/GermEval21_Toxic_/GermEval21_Toxic_.csv'\n",
        "data=pd.read_csv(filename)\n",
        "\n",
        "data.set_index('comment_id')\n",
        "# df1 is the subtask1 for toxic\n",
        "df1=data[['comment_text','Sub2_Engaging']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRAnH-vmMLcu"
      },
      "source": [
        "#Model one ANN- Split for training and validation and test set\n",
        "# in future make a k-fold cross validation with sklearn\n",
        "train, test = train_test_split(df1, test_size=0.2)\n",
        "val, test = train_test_split(test, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')\n",
        "\n",
        "train_ds=train['comment_text'].astype(str).to_numpy()\n",
        "val_ds=val['comment_text'].astype(str).to_numpy()\n",
        "test_ds=test['comment_text'].astype(str).to_numpy()\n",
        "\n",
        "whole_dataset=df1['comment_text'].astype(str).to_numpy()\n",
        "whole_labels=df1['Sub2_Engaging'].astype(int).to_numpy()\n",
        "\n",
        "train_labels=train['Sub2_Engaging'].astype(int).to_numpy()\n",
        "val_labels=val['Sub2_Engaging'].astype(int).to_numpy()\n",
        "test_labels=test['Sub2_Engaging'].astype(int).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVH8Zjm_NC32"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_ds,train_labels))\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((val_ds,val_labels))\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((val_ds,val_labels))\n",
        "\n",
        "whole_ds=tf.data.Dataset.from_tensor_slices((whole_dataset,whole_labels))\n",
        "\n",
        "train_ds = train_ds.batch(BATCH_SIZE)\n",
        "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
        "\n",
        "# PREFETCH\n",
        "\n",
        "train_ds = train_ds.shuffle(5).prefetch(buffer_size=AUTOTUNE)\n",
        "valid_ds = valid_ds.shuffle(5).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fyYbDyRNjuo"
      },
      "source": [
        "#Model 1 Sentence_encoder\n",
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer( \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\", name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\", name='multi_BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aVGFLwzNw91"
      },
      "source": [
        "classifier_model = build_classifier_model()\n",
        "tf.keras.utils.plot_model(classifier_model)\n",
        "\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = [tf.metrics.BinaryAccuracy()]\n",
        "\n",
        "epochs = 50\n",
        "steps_per_epoch =  tf.data.experimental.cardinality(train_ds).numpy()\n",
        "# tf.data.experimental.cardinality(np.array(train_ds)).numpy()\n",
        "# len(train)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 1e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "# monitoring the val_loss is the best way as monitoring actual training loss results in overfitting ..\n",
        "mcp_save = ModelCheckpoint('bert1.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "early_stop=EarlyStopping(monitor='val_loss', patience=3, verbose=0,restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXxOm9iAOG_A"
      },
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A03uEnZAOQx3"
      },
      "source": [
        "history = classifier_model.fit(train_ds,\n",
        "                               validation_data=valid_ds,\n",
        "                               epochs=epochs,callbacks=[mcp_save,early_stop], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zbHMqu3Ogoj"
      },
      "source": [
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='val')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['binary_accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_binary_accuracy'], label='val')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-s7CGVTOq17"
      },
      "source": [
        "#model_1!!!!!\n",
        "\n",
        "#load saved best weights to the model\n",
        "classifier_model.load_weights('bert1.h5')\n",
        "#load weights trained with gpu\n",
        "# classifier_model.load_weights(r'/content/drive/MyDrive/Dataset/GermEval21_Toxic_/bert.h5')\n",
        "# predict probabilities for test set and whole dataset\n",
        "yhat_probs = classifier_model.predict(test['comment_text'], verbose=0)\n",
        "yhat_classes=tf.cast(tf.round(tf.sigmoid(yhat_probs)), tf.int32)\n",
        "\n",
        "yhat_df1probs = classifier_model.predict(df1['comment_text'], verbose=0)\n",
        "yhat_df1classes=tf.cast(tf.round(tf.sigmoid(yhat_df1probs)), tf.int32)\n",
        "\n",
        "df1['predictions_LaBse']=yhat_df1classes.numpy()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvjkAoX1RI_l"
      },
      "source": [
        "#metrics for the small test set.....  5 excamples\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "print('Metrics for test set ONLY 5 examples\\n')\n",
        "accuracy = accuracy_score(test['Sub2_Engaging'], yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(test['Sub2_Engaging'], yhat_classes,average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(test['Sub2_Engaging'], yhat_classes,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['roberta_predictions'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uncnh9OVHed"
      },
      "source": [
        "#metrics for Model 1--- sentence encoder!!!!!\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "print('Metrics for Setence _econder\\n')\n",
        "accuracy = accuracy_score(df1['Sub2_Engaging'],df1['predictions_LaBse'])\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(df1['Sub2_Engaging'], df1['predictions_LaBse'],average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(df1['Sub2_Engaging'], df1['predictions_LaBse'],average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['roberta_predictions'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4Dg7xEiROr5"
      },
      "source": [
        "#initialize transformer embeddings \n",
        "transformer_embedding = TransformerDocumentEmbeddings('xlm-roberta-base')  #xlm-large is too big for memory\n",
        "\n",
        "#initialize flair embeddings\n",
        "flair_embedding_forward = FlairEmbeddings('de-forward');\n",
        "flair_embedding_backward = FlairEmbeddings('de-backward');\n",
        "document_embeddings = DocumentPoolEmbeddings([flair_embedding_forward, flair_embedding_backward]) ;\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIPpA5OXRYDV"
      },
      "source": [
        "#Model 2-- flair embeddings\n",
        "df1['flair_embeddings']= \" \"\n",
        "\n",
        "#create features for tweets_training for Model 2 FLair embeddings\n",
        "for i in range(0,len(df1)):\n",
        "\n",
        "    # create a sentence\n",
        "    sentence = Sentence(df1['comment_text'][i])\n",
        "\n",
        "    # embed the sentence\n",
        "    document_embeddings.embed(sentence)\n",
        "\n",
        "    embedding = sentence.embedding.cpu()\n",
        "    \n",
        "    #save vector as numpy\n",
        "    embedding = embedding.detach().numpy()\n",
        "    \n",
        "    #save vector as pandas dataframe\n",
        "    embedding = pd.DataFrame(embedding)\n",
        "    \n",
        "    #make list out of sentence\n",
        "    embedding = embedding[0].tolist()\n",
        "\n",
        "    #add the embedding vector to the column of stacked embeddings\n",
        "    df1['flair_embeddings'][i] = embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJUg5yqaRiaD"
      },
      "source": [
        "#create features for tweets_training for Model 3\n",
        "\n",
        "df1['roberta_embeddings']= \" \"\n",
        "\n",
        "for i in range(0,len(df1)):\n",
        "\n",
        "    # create a sentence\n",
        "    sentence = Sentence(df1['comment_text'][i])\n",
        "\n",
        "    # embed the sentence\n",
        "    transformer_embedding.embed(sentence)\n",
        "\n",
        "    embedding =sentence.embedding.cpu()\n",
        "\n",
        "    #save vector as numpy\n",
        "    embedding = embedding.detach().numpy()\n",
        "    \n",
        "    # #save vector as pandas dataframe\n",
        "    embedding = pd.DataFrame(embedding)\n",
        "    \n",
        "    # #make list out of sentence\n",
        "    embedding = embedding[0].tolist()\n",
        "\n",
        "    # #add the embedding vector to the column of stacked embeddings\n",
        "    df1['roberta_embeddings'][i] = embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuQ0ynUR2ee"
      },
      "source": [
        "model2 = svm.SVC(C=1, kernel='linear', gamma=1)\n",
        "model2.fit(list(df1['flair_embeddings']),df1['Sub2_Engaging'])\n",
        "\n",
        "\n",
        "model3 = svm.SVC(C=10, kernel='linear', gamma=1)\n",
        "model3.fit(list(df1['roberta_embeddings']),df1['Sub2_Engaging'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tz9QAUFORFN"
      },
      "source": [
        "\n",
        "\n",
        "model2_pred = model2.predict(list(df1['flair_embeddings']))\n",
        "df1['predictions_flair']=model2_pred\n",
        "\n",
        "\n",
        "model3_pred = model3.predict(list(df1['roberta_embeddings']))\n",
        "df1['predictions_roberta']=model3_pred\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AthHvtIDKFF2"
      },
      "source": [
        "#Metrics for FLair\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "\n",
        "print('Metrics for FLair\\n')\n",
        "accuracy = accuracy_score(df1['Sub2_Engaging'], df1['predictions_flair'])\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(df1['Sub2_Engaging'], df1['predictions_flair'],average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(df1['Sub2_Engaging'], df1['predictions_flair'],average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['roberta_predictions'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IUiuclbdQZq"
      },
      "source": [
        "#Metrics for roberta\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "print('Metrics for Roberta\\n')\n",
        "accuracy = accuracy_score(df1['Sub2_Engaging'], df1['predictions_roberta'])\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(df1['Sub2_Engaging'], df1['predictions_roberta'] ,average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(df1['Sub2_Engaging'], df1['predictions_roberta'] ,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['roberta_predictions'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCRaz2epXImd"
      },
      "source": [
        "#do majority voting -- Ensemble\n",
        "df1['Ensemble'] = df1[['predictions_LaBse','predictions_flair' ,'predictions_roberta']].mode(axis=1)[0]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wOqwmcuXwqI"
      },
      "source": [
        "#Metrics for Ensemble\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "print('Metrics for Ensemble\\n')\n",
        "accuracy = accuracy_score(df1['Sub2_Engaging'], df1['Ensemble'])\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(df1['Sub2_Engaging'], df1['Ensemble'] ,average='macro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(df1['Sub2_Engaging'], df1['Ensemble'] ,average='macro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "\n",
        "# f1 = f1_score(df1['Sub1_Toxic'], df1['roberta_predictions'])\n",
        "# print('F1 score: %f' % f1)\n",
        "\n",
        "if precision+recall > 0:\n",
        "  f1_score = 2*precision*recall/(precision+recall)\n",
        "print('F1 score: %f' % f1_score)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}